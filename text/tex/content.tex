%=========================================================================

%- potreba mit 10-20 vysazenych stran / 20-40 normostran
%- na jednu plne potistenou stranu se vleze 1,65 (s nadpisem chapter) nebo 2,36 (bez nadpisu) normostran

% Setting the depth of sections numbering
\setcounter{secnumdepth}{2}
% Setting the depth of sections to appear in the table of contents.
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}


%=========================================================================
%=========================================================================
\chapter{Introduction}
% 1 - 2 ns

- coincidence rangefindrer: \url{https://en.wikipedia.org/wiki/Coincidence_rangefinder}
- proc pouzit opticky system 0 hloubkove senzory maji maly dosah a nefunguji ve venkovnim prostredi
- Radar (radio detection and ranging) is a device with the active emission of the radio signal. To us it for localization of the object either single radar measuring both the angle and the distance to a target or multiple radars measuring only the distance to a target and triangulating its position can be used \cite{toomay2012radar}\cite{BistaticRadar}. +sonar, laser

- vyhody/nevyhody systemu
/ princip rage finderu vyuzival uz thales https://en.wikipedia.org/wiki/Thales

OLS = Optical Localization System
- zminit problematiku zastaniceni a rektifikace a odkaz na prislusnou kapitolu

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Background} \label{txt:background}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Problem formulation}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Related work}
% 1 - 2 ns

The principle of the optical localization of objects is well known for quiet a long time (the first optical range finders emerged in 18th century\cite{reviews2013study}) and nowadays is still mainly used for military purposes and in robotics. However the professional systems aiming on automatic localization of aerial (as well as terrestrial and underwater) targets mostly rely on active devices, such as radars or sonars and there is very little use of pure passive optical devices (such as RGB cameras, thermal imaging cameras, etc.). The reason might be the complexity of the whole solution or the operation constrained by the weather conditions (see Section \ref{txt:background}).

On the other hand since the application of multiple view geometry has been one of hot topics in the computer vision during the past decade, many R\&D groups (specializing mostly on robotics) attempt to base their solutions on multi camera optical localization.

One of a widely used approaches is to set up so called \textit{intelligent space}\cite{intelligentSpace}, the bounded area under surveillance of multiple cameras reporting to the the central system. In \cite{Multi-Camera_Sensor_System_for_3D_Segmentation} multiple terrestrial robots are detected, tracked and localized by the multi camera system. For all cameras the intrinsics and extrinsics are known beforehand and do not change over time.  A similar approach is used also in \cite{Localization_and_Geometric_Reconstruction_of_Mobile_Robots} but this system uses the robot's odometers as well in order to improve the resulting position estimated by the optical system. The system presented in \cite{A_3D_visual_localization} then relies on four ordinary RGB cameras with known parameters (intrisics, extrinsics) to estimate the position of the easy to detect and static object using the Perpendicular Foot Method algorithm.

Even though all of the aforementioned approaches utilize the ordinary RGB cameras for detection and tracking and in general estimate the location of the object using the triangulation algorithm, they all assume the object moves strictly within the specified bounded region and they rely on fixed positions of the cameras. Thus they do not have to deal with the imprecisions arose from the uncertainty of the current camera pose estimation and with the problems of transferring the target. 

As for the detection and tracking of the distant aerial targets, one of the most recent approaches was presented by Rozantsev et al.\cite{DBLP:journals/corr/RozantsevLF14}. The algorithm combine both the appearance and motion clue in order to distinguish the object from the complex background.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Thesis organization}

- napad: rozdelit uvod do pokapitol
	- background
	- related work
	- problem formulation
	- system overview
	- thesis organization

- Slouží k zasazení řešené problematiky do širšího kontextu
- o optickych dalkomerech
- vyhody systemu (pasivni radar) oproit aktivnim systemum
- vyuziti

- v podobě stručného obsahu jednotlivých kapitol definuje strukturu písemné práce.


-- dalsi kapitoly --

 formulaci cíle práce, charakteristiku současného stavu
 řešené problematiky a teoretická a odborná východiska řešených problémů.


%=========================================================================
%=========================================================================
\chapter{System overview} \label{txt:system_overview}
% 2 - 4 ns

This section presents the main hardware building blocks of the OLS, the camera stations, and describes the basic pipeline which the system must perform in order to localize a given object. From the implementation point of view, the OLS is built on the ROS framework (see Section \ref{txt:implementation}) which present certain conventions, most importantly the orientation of the coordinate frame which will be used throughout the document (see Figure \ref{fig:frame_convention}).

%% ROS frame conventions
\begin{figure}[htb]
	\centering
	\includegraphics[width=4cm]{fig/frame_convention.pdf}
	\caption{A frame orientation convention used throughout the work, adopted from ROS. A right handed frame with X axis aiming forward, Y axis left and Z axis up.}
	\label{fig:frame_convention}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Camera stations}

The main building block of the OLS system is a \texttt{camera station} (CS). CS consists of the \texttt{camera unit} (CU), which is a collection of hardware necessary for capturing the images, manipulating the position of the camera and estimating the geographical coordinates of the CS (for detailed description see Section \ref{txt:camera_unit}), and the computation unit processing the data (a PC or other device). In general the OLS is designed to work with arbitrary number of CSs (of course there must be at least two units) and there are two types of CSs:

\paragraph{overview station} Only one of the CSs is selected to become the overview station. Its main objective is to scan the surrounding environment, discover the aerial objects, distribute the information to the tracking stations and prospectively take part in the tracking phase. The station dispose of the camera equipped with the zooming lens allowing for scanning greater distances and the PC serving the purpose of the main entry point for the human operator (see Section \ref{txt:hardware}).

\paragraph{tracking station} All other stations become tracking stations which scan the surrounding environment, detect the aerial objects and perform tracking. All cameras ar equipped with the fixed focal length lenses. \\

As for the network topology, the OLS is base don a star pattern where each tracking station communicates only with the overview station (see Section \ref{txt:hardware}). The positional organization of the CSs depends on the number of tracking stations, which should always be placed so that their positions projected to the horizontal plane would form a regular polygon with the overview unit in the middle (see Figure \ref{fig:system_overview}), however in more complex environments this condition does not have to strictly hold (see Figure \ref{fig:spilberk_camera_units}).
	
%% Schema of camera units organized in triangle.
%% Use case of camera units - placed on map of Brno's Spilberk
\begin{figure}[htb]
	\centering
	\begin{minipage}{.45\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/system_overview.pdf}
		\captionof{figure}{The schematic view of the ideal-case organiztaion of all camera units where all three tracking units make up an equilateral triangle with the overviw unit in the middle. The inital orientation of the tracking units is shown as well as their FOVs $\gamma_{1-3} = 38^{\circ}$ (assuming camera Prosilica GT 1290C and a lense with equivalent 50mm focal length).}
		\label{fig:system_overview}
	\end{minipage}
	\hfill
	\begin{minipage}{.48\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/spilberk_camera_units.pdf}
		\captionof{figure}{An use case scenario showing the organization of the camera units within the system set to protect a real world area (the castle Spilberk in Brno, Czech Republic). As can be seen in the Figure, given the possibilities of the protected area (some CUs mounted on the rooftops) the CUs do not form an equilateral triangle, neither is the overview unit positioned in the middle.}
		\label{fig:spilberk_camera_units}
	\end{minipage}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{System pipeline}

Before the 3D location of the given target is estimated the system must perform several operations. After powering on the system all camera stations start scanning the surrounding environment. The scanning process is autonomous, however should the operator decide to interfere the overview unit (through the GUI and peripheral devices) is used in order to control the camera. 

Once some of the stations detects an aerial object, it saves the visual information distinguishing the object and notifies the overview station. The overview station selects the the best candidate(s) to track the newly discovered object and notifies the chosen unit with the essential data about the object (direction in which it was spotted, visual clues, global id, etc.).

The other unit(s) must first detect the newly spotted object. If it succeeds it initiates the tracking. The overview unit then performs the triangulation and estimates the 3D location of the target.


%=========================================================================
%=========================================================================
\chapter{Camera unit} \label{txt:camera_unit}
% 2 - 4 ns

A camera unit is a component of a camera station consisting of a hardware necessary to estimate the absolute geographical position of a station, absolute orientation, relative position and orientation with regards to the rest of the stations and a hardware for positioning a camera and capturing an image stream (more in Section \ref{txt:devices}). A 3D location of a camera as well as the direction of the optical axis must be known for each captured frame, thus a model corresponding to the real hardware must be designed (see Section \ref{txt:model}).

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Devices} \label{txt:devices}

A camera unit (see Figure \ref{fig:camera_unit_photo_model}) consists of a surveying tripod providing a solid base on which a manipulator (P\&T unit\footnote{From english Pan and Tilt.}) together with the LED target for stationing (see Section \ref{txt:stationing_and_rectification}) is mounted as well as all of the sensors (a GPS sensor, an inclinometer, a camera).

%% A photo of a camera unit and an rviz model.
\begin{figure}[htb]
	\centering
	\includegraphics[width=10cm]{fig/camera_unit_photo_model.jpg}
	\caption{A photograph of the upper part of the camera unit consisting of a manipulator Flir PTU-D46-70 with the aluminum mount carrying a camera Prosilica GT 1290C (left) and a corresponding 3D model created for Gazebo simulator (right, see Section \ref{txt:application_of_gazebo}).}
	\label{fig:camera_unit_photo_model}
\end{figure}

\paragraph{manipulator Flir PTU-D46-70} A professional manipulator PTU-D46-70\footnote{A website of product Flir PTU-D46-70: \url{http://www.flir.com/mcs/view/?id=53712}} produced by a well established manufacturer Flir is used (see Figure \ref{fig:prosilica_gt1290c_flir_ptud4670}). As compared to the other professional manipulators this is an entry level device consisting of two stepper motors (pan and tilt axes). the device is capable of maximum angular speed of $60^{\circ}/s$ with the resolution of $0.003^{\circ}$ while the payload must not exceed 4.08 kg \cite{Flir_ptud4670}. The operational range is limited to $[-180^{\circ}, 180^{\circ}]$ in azimuth and $[-47^{\circ}, 80^{\circ}]$ in elevation. The manipulator incorporates no position feedback, the position is inferred from the number of steps and the current resolution, thus it is necessary not to overload the manipulator, otherwise it could loose synchrony and report wrong position. 

\paragraph{camera Prosilica GT 1290C} A camera Prosilica GT 1290C\footnote{A website of product Prosilica GT 1290C: \url{https://www.alliedvision.com/en/products/cameras/detail/1290-1.html}} is an industrial camera manufactured by the company Allied Vision (see Figure \ref{fig:prosilica_gt1290c_flir_ptud4670}). It is a color camera equipped with CCD sensor (type 1/3'') with the resolution of $1280 \times 960$ px and support for $33.3$ FP and it communicates through gigabit Ethernet \cite{Prosilica_gt1290c}. What is important the camera natively supports the PTP protocol for precise time synchronization which is a crucial feature in each application relying on stereo vision as it is capable of time synchronization devices within the range of nanoseconds \cite{PTP}.

%% Photos of Flir and Prosilica.
\begin{figure}[htb]
	\centering
	\includegraphics[width=10cm]{fig/prosilica_gt1290c_flir_ptud4670.jpg}
	\caption{The photographs of the manipulator Flir PTU D46-70 (left) and the camera Prosilica GT 1290C (right).}
	\label{fig:prosilica_gt1290c_flir_ptud4670}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Model} \label{txt:model}

A camera unit model is based on the kinematic chain consisting of six joints and five links corresponding to the distance between the separate part of the tripod and the manipulator (see Figure \ref{fig:camera_unit_kinematic_chain}). The starting joint \texttt{ground} itself is dependent on the reference location (let's call it \texttt{world}) which represents the origin of the global coordinate frame. The transformation between the \texttt{world} and \texttt{ground} reflects the positioning and heading of the given manipulator within the environment (which is estimated during the stationing process, see Section \ref{txt:stationing_and_rectification}).

The kinematic chain is designed as the composition of transformation matrices where a single joint can be located by applying the Euclidean transformation on the position of the joint it is dependent on. For instance the transformation matrix $M_{cam}$ of the joint \texttt{camera} can be derived as:

\begin{equation}
M_{cam} = M_{ele}T_{cam}R_{Z_{cam}}R_{X_{cam}}R_{Y_{cam}},
\end{equation}

where $M_{ele}$ is the transformation matrix of the joint \texttt{ele} which the joint \texttt{camera} is dependent on. 

%% A schema of the camera unit - kinematic chain.
\begin{figure}[htb]
	\centering
	\includegraphics[width=14cm]{fig/camera_unit_kinematic_chain.pdf}
	\caption{A schematic view of a kinematic chain of a camera unit with the joints depicted by the yellow circles with black crosses. The sizes of all components necessary to specify the translation matrices between consecutive joints are shown as well. Note that this is a rear view, i.e. the optical axis of the camera is seen from behind. Thus the joints \texttt{camera} and \texttt{focus} overlap as they both lie on the optical axis.}
	\label{fig:camera_unit_kinematic_chain}
\end{figure}


%=========================================================================
%=========================================================================
\chapter{Sensitivity analysis} \label{txt:sensitivity_analysis}
The precision of the system can be defined in the means of the frame-by-frame Euclidean distance between the estimated location and the real (ground truth) location of the given target. The precision is impacted by multiple independent factors, thus it is essential to perform the sensitivity analysis in order to discover and prospectively alleviate the most prominent contributors of the overall error. 

- vysvetleni typu chyb:
	- system error (systemova chyba)
		- nesoulad modelu CU s realnou konstrukci
		- nespravne mereni heading
		- detekce a tracking ?
	- uncertainty of the input of the system
		- GPS mereni
		- data inklinometru
	
- vysvetlit, ze se budeme snazit potlacit jen nektere chyby (neresime treba nepresnost mezi modelem a realnou CU z hledsiak translaci mezi klouby)

- rozdeleni na chybu rotace a translace
	- nepresnost v rotaci je daleko zavaznejsi, nez nepresnost v translaci
	- priklad a obrazek vlivu nepresnosti o x mrad na lokalizaci cile ve vzdalenosti y m/km

- zdroj chyb:
	- detekce
	- tracking
	- GPS pozice
	- natoceni vuci severu
	- rozliseni PTU
	- model PTU - translace mezi klouby
	- model PTU - rotace mezi klouby
	- uchyceni kamery
		- rotace podle opticke osy
		- rotace podle osy azimutu
		- rotace podle osy elevace
		
%- vysvetleni input a output systemu a pojmu sensitivity analysis (http://samples.sainsburysebooks.co.uk/9780470725177_sample_389211.pdf)

%=========================================================================
%=========================================================================
\chapter{Stationing and rectification} \label{txt:stationing_and_rectification}
% 2,5 - 3,5 ns

As explained in section \ref{txt:sensitivity_analysis} the precision of the whole system is dependent on on the uncertainty of the system input as well as on the imprecision of the camera unit construction. The process of stationing aims to alleviate the uncertainty of the system input while the main purpose of the rectification is to reduce the difference between the real camera unit and its model.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Stationing}

Since the stationing is considered to be already working subsystem of the whole project (and thus is not dealt with within the scope of this work) only the main principle will be briefly described. The stationing is composed of two parts: finding the geographical north and finding the relative azimuthal and elevation angles between each pair of camera units.

%.........................................................................
%.........................................................................
\subsection{Geographical north} \label{txt:geographical_north}

Though it is common practice to estimate the heading\footnote{Heading is the term used to describe the angle between the torso of the human body and the geographical north \cite{Henriksson648760}} using the magnetometer, this device is unsuitable for this project since the accuracy of the concurrent professional class magnetometers is insufficient (see Section \ref{txt:sensitivity_analysis}). For instance the accuracy of the magnetometers meant for compassing applications produced by Honeywell, the multinational company focusing on aerospace systems, range from hundreds to thousands of milliradians \cite{Honeywell:compassing_catalog}.

In order to find the orientation of each camera unit placed in the outdoor environment, distinctive landmarks (created either by human or nature) with known geographical positions are used. For each such a landmark the manipulator is rotated so that the optical axis of the camera would intersect that landmark and both the azimuth and elevation value is registered. Using triangulation the geographical position of the camera unit is derived. 

Different possible approach takes advantage of the celestial objects, such as the moon, sun or stars for which the current geographical position is known as well. Nevertheless this approach can only be used between the sunset and the dawn.

%.........................................................................
%.........................................................................
\subsection{Relative azimuth and elevation}

To further reduce the impact of the uncertainty of the system input produced by the GPS (see Section \ref{txt:sensitivity_analysis}) and the system error given by the imprecision of the heading estimation (see Section \ref{txt:geographical_north}) it is convenient to find the relative position of each camera unit with regards to the rest of the camera units.

The information about the geographical position of all camera units as obtained from the GPS sensors is distributed across the whole system. Each pair of camera units then automatically performs the following:

\begin{enumerate}
	\item Set the azimuth and elevation of the manipulator so that the optical axis of the camera would intersect the expected location of the LED target of the other camera unit.
	\item Using the visual clue adjust the azimuth and elevation so that the optical axis of the camera would intersect the center of the LED target of the other unit (see Figure \ref{fig:stationing_aiming}).
	\item Save the current azimuth and elevation values of both camera units and use those values to update the model of the system.
\end{enumerate}

%% Stationing process of one pair of the camera units
\begin{figure}[htb]
	\centering
	\includegraphics[width=14cm]{fig/stationing_aiming.png}
	\caption{The schema of stationing process where two camera units attempt to align the optical axes of their cameras so that they would intersect the LED target of the other unit.}
	\label{fig:stationing_aiming}
\end{figure}


%.........................................................................
%.........................................................................
\subsection{Horizontality}
Since the camera unit is expected to be placed in an unknown outdoor terrain, it will never stand on an ideally horizontal surface. Thus it is necessary to either ensure that the unevenness of the surface is compensated by the suitable setting of the camera unit's stand or both the side tilt and front tilt angles of the stand must be estimated and integrated to the model of the given camera unit. For these purposes the inclinometer attached to the base plane of the camera unit (see Section \ref{txt:camera_unit}) is used.


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Rectification}

The process of rectification serves the purpose of reducing the system error caused by the imprecise attachment of the camera to the manipulator. The model of the camera unit assumes that the camera is precisely attached to the manipulator so that the camera image sensor is positioned perpendicular to the azimuthal axis and the rows of the image sensor are parallel to the elevation axis (i.e. the camera is not rotated along the optical axis).

Regarding these requirements the rectification consists of three parts: rotation along the optical axis, rotation along the azimuthal axis, finding the default elevation angle.


%.........................................................................
%.........................................................................
\subsection{Rotation along the optical axis}

A custom made metal mount is attached to the bottom side of the camera. The mount is then attached to the manipulator using two opposing round tenons enabling for the rotation of the mount (together with camera) along the axis parallel to the optical axis of the camera (see Figure \ref{fig:rect_model_front_view}).

%% The front view of the model of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_model_front_view.png}
	\caption{Front view of the top part of the camera unit. The red arrow shows the possible rotation of the camera along the axis parallel to the optical axis.}
	\label{fig:rect_model_front_view}
\end{figure}

In this part the rectification target with three parallel horizontal black lines is used. Each line has different width so that the operator can select the most suitable one (given the distance of the target, ambient lighting conditions, etc.) As the first step a surveying automatic level is used to rotate the target so that the black lines become horizontal. Then the camera is pointed approximately to the center of the target. The camera image stream is blended with the same stream mirrored across the vertical axis. The operator then manually rotates the camera so that the black lines in this blended image stream appear visually aligned (see Figure \ref{fig:rect_mirrored_stream}). Once set, the mount with the camera is fixed to the manipulator using two set screws.

%% Rectification of the rows of the camera image sensor - original stream blended with the mirrored stream
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_mirrored_stream.png}
	\caption{A blended image stream from the camera before (left) and after (right) rotating the camera along the optical axis to the correct position.}
	\label{fig:rect_mirrored_stream}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Rotation along the azimuthal axis}

The mount can still rotate along the axis parallel to the azimuthal axis (see Figure \ref{fig:rect_model_top_view}). It is necessary to ensure that the optical axis of the camera is perpendicular to the elevation axis. 

%% The top view of the model of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_model_top_view.png}
	\caption{Top view of the top part of the camera unit. The red arrow shows the possible rotation of the camera along the axis parallel to the azimuthal axis.}
	\label{fig:rect_model_top_view}
\end{figure}

The same target from the first part of the rectification is used, but two black crosses are added to the selected horizontal black line. The distance $d_{ao}\ m$ between two crosses equals to the distance between the azimuthal and optical axis (which is known from the engineering design, see Figure \ref{fig:rect_azi_axis}). 

%% Photograph of the camera unit with the telescope mounted on the top
%% Screenshot of the digital crosshair and the offset
\begin{figure}[htb]
	\centering
	\begin{minipage}{.42\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/rect_telescope.png}
		\captionof{figure}{A telescope mounted on top of the manipulator. A person looking through a telescope sees a crosshair - a tip of a triangle.}
		\label{fig:rect_telescope}
	\end{minipage}
	\hfill
	\begin{minipage}{.54\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/rect_azi_axis.png}
		\captionof{figure}{Rectification target with the pairs of black crosses. The two crosses in a pair are $d_{ao}\ m$ appart. A digital corsshair is displayed in order to find the horizontal offset $d_{h}$.}
		\label{fig:rect_azi_axis}
	\end{minipage}
\end{figure}

A military optical monocular telescope (see Figure \ref{fig:rect_telescope}) is mounted on top of the manipulator. The optical axis of the telescope intersects the azimuthal axis, it is perpendicular to it and it intersects the left cross of a given pair on the rectification target. The camera is rotated so that its optical axis (represented by the digital crosshair) intersects the right cross on the target and then is fixed using set screws. As the screws are tightened the camera is unintentionally rotated a bit again which causes the visual offset between the crosshair and the cross on the target. The offset expressed in pixels is recorded and transformed to the default angle $\beta$ expressed in milliradians (see Figure) of rotation along Z-axis of the joint \texttt{camera} in the camera unit model (see Section \ref{txt:camera_unit}):

\begin{equation*}
\begin{aligned}
\beta &= \arccos\frac{focal\_length}{offset}, \\
offset &= pixel\_offset * pixel\_size
\end{aligned}
\end{equation*}

%% Geometry schema showing how to calculate angle beta - pixel offset
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_pixel_offset.pdf}
	\caption{The top view schema of a rectification target being projected to the image sensor of the camera. The value of the angle $\beta$ is one of the output of the rectification process.}
	\label{fig:rect_pixel_offset}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Finding the default elevation angle}

Given the application of the system the camera is expected to mainly observe the sky. Considering the limited elevation range of the manipulator \texttt{Flir PTU D46-70} (see Section \ref{txt:camera_unit}), the camera must be mounted to the manipulator with the default elevation angle approximately $-60^{\circ}$. However, after fixing the camera it is necessary to find default elevation angle precisely. 

For this purpose a pair of rectification targets, which consist of vertical black and white lines representing the marks of a ruler. The targets are positioned in a row with the distance of a few meters so that the front target would overlap approximately half of the rare target when observed from the camera. Both targets must be rotated so that the lines become horizontal, then the operator manually adjusts the elevation of the manipulator so that the digital crosshair would intersect the same mark on both targets where the two marks form a straight line (see figure \ref{fig:rect_default_elevation_angle}). Once found the current elevation angle is recorded and integrated to the model of the camera unit as an angle of rotation along the Y-axis of the joint \texttt{camera} (see Section \ref{txt:camera_unit}).

%% The rectification target for finding the default elevation angle.
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_default_elevation_angle.png}
	\caption{Front target of a pair of the rectification targets used to find a default elevation angle (left). A screenshot from the image stream of the camera with the crosshair focused on a row where the marks of the rulers align (right).}
	\label{fig:rect_default_elevation_angle}
\end{figure}


%=========================================================================
%=========================================================================
\chapter{Detection and tracking} \label{txt:detection_and_tracking}
% 4,5 - 9 ns

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Object model}

Choosing a suitable representation of an object of interest is is a first step when designing a tracker. It is a crucial task since the choice of the features determines the type of the final tracker due to the strong relationship between the algorithm and the representation of the object and it can significantly impact the overall performance of the tracker. There are two main representations, the \textit{shape} and the \textit{appearance} \cite{Yilmaz:2006:OTS:1177352.1177355}.

\paragraph{shape representation} 
This category covers the use of points, geometric shapes, silhouette and contour, articulated models and skeletal models. The point representation is not suitable for OLS since the distant aerial objects appear relatively small in the image and might not provide enough distinctive points. The geometric shapes seem suitable as long as the aerial object is far enough so that it could be approximated by a primitive geometric shape. Silhouettes and contours are mostly used for tracking non rigid objects (which is not the case of OLS), neither articulated model is needed to describe an UAV. Considering the small size of the object of interest, a skeletal model would most probably degrade to a single line or a point.

\paragraph{appearance representation} 
The second category covers the use of templates, active appearance models and multiview appearance models. Active appearance models seem promising since they simultaneously model the object shape and appearance, however there is a need for an offline training phase. The templates combine the spatial and appearance information and can scale well to support varying object size (UAV approaching or flying away). A multiview appearance model might perform even better since it is designed to be robust against variation of the appearance of one object due to the change of its orientation.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Detection}

Before a tracking of the object can be initiated the object of interest must first be discovered, which is the main task of object detectors. Even though the OLS allows a human operator to interfere and manually select an object for tracking, the system should be able to perform fully autonomously as well. The approaches to detect an object can be divided into two categories based on the appearance of the object, where the first group covers the cases where the system already posses a strong information about the objects since they incorporate artificial landmarks, while the second group relies merely on the natural appearance and has only limited or no prior knowledge about the objects \cite{Multi-Camera_Sensor_System_for_3D_Segmentation}. Since the OLS aims on tracking unknown UAVs the first group of approaches is out of question.

A couple of approaches can be distinguished among the detectors using natural appearance \cite{Yilmaz:2006:OTS:1177352.1177355}:

\paragraph{detection of points} This approach goes with the shape representation of the object given by the keypoints. Among others the Harris corner detector or SIFT and SURF descriptors are widely used.

\paragraph{background modeling} Under the assumption an observed scene does not change rapidly its appearance can be learned resulting in a background model. The task of the object detector is then to estimate for each subregion or even each pixel whether it belongs to a background or to a foreground (i.e. the object of interest). A widely used approach is to model each pixel as a mixture of Gaussians (in order to support a periodically changing background). Another possible solution is to model intensity variations of separate pixels as the states (including both \textit{background} and \textit{foreground} state) and then with the help of Hidden Markov Models to estimate the state a given pixel is currently in. Since it is possible to enhance these algorithms to allow for the camera motion (resulting in the change of the camera view) they seem suitable for the OLS system. 

\paragraph{supervised classification} If the (coarse) class of the objects to be tracked is known beforehand and a proper dataset is available, a classifier can be trained to detect the objects. Among others the neural networks or adaptive boosting methods are widely used. Since the OLS aims to localize UAVs, the utilization of a classifier seems like a suitable solution.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Tracking}

The main purpose of the tracker is to iteratively estimating the trajectory of the tracked object from frame to frame and regarding the approaches the trackers can be divided into three main categories \cite{Yilmaz:2006:OTS:1177352.1177355}:

\paragraph{point tracking} 
The tracker detects the keypoints in each frame and, select those representing the object and computes the transformation from the previous frame.

\paragraph{kernel tracking} 
This approach corresponds to a representation of an object using a template, where kernel is the description of both shape and appearance.

\paragraph{silhouette tracking}
Silhouettes representing the object can be tracked either by shape matching or contour evolution. \\

Since the representation by a template (or multiple templates) suits conditions of the OLS the best, an approach Tracking Learning Detection belonging to the \textit{kernel tracking} category was chosen to perform the tracking of the object of interest. TLD is one of the best performing state-of-the-art algorithms developed for tracking general objects and its capabilities seem promising for the needs of OLS. For detailed description of the algorithm see Section \ref{txt:tracking_learning_detection}.

%.........................................................................
%.........................................................................
\subsection{Tracking Learning Detection} \label{txt:tracking_learning_detection}

The Tracking Learning Detection (TLD) \cite{Kalal:2012:TRA:2225045.2225082} is an algorithm designed for performing so called long-term tracking, a robust tracking of an object which might change its appearance, be temporarily occluded by closer objects or temporarily completely disappear from the scene. Since this task cannot be achieved solely by a tracker nor by a detector, the TLD aims to combine the strengths of the detection and tracking algorithms by combining their results. Furthermore the algorithm incorporates the online adaptation subsystem capable of learning the new appearances of the tracked object in the course of the tracking.

A conceptual diagram of the TLD algorithm is shown in Figure \ref{fig:tld_block_diagram}. The \texttt{tracking} component tracks the object and for each frame produces the new position. It expects that object does not disappear (occlusion, out of FOV) from the scene and if it does the tracker fails. The \texttt{detection} component performs full scanning of the image for each frame. It detects the object and if needed it reinitializes the tracker. The \texttt{learning} component is capable of generating new appearances of the tracked object and thus improving the performance of the detector. 

The object itself is modeled as a set of patches, each patch being already learned appearance represented by the rectangular bounding box around the object rescaled to a normalized resolution of 15 x 15 pixels. The similarity of the patches is given by the normalized cross-correlation.

%% TLD block diagram
%% TLD PN learning block diagram
\begin{figure}[htb]
	\centering
	\begin{minipage}{.34\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/tld_block_diagram.png}
		\captionof{figure}{A diagram of the main TLD components. Image is adopted from \cite{Kalal:2012:TRA:2225045.2225082}.}
		\label{fig:tld_block_diagram}
	\end{minipage}
	\hfill
	\begin{minipage}{.63\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/tld_pn_learning_block_diagram.png}
		\captionof{figure}{A diagram of the PN learning process. Image is adopted from \cite{Kalal:2012:TRA:2225045.2225082}.}
		\label{fig:tld_pn_learning_block_diagram}
	\end{minipage}
\end{figure}

\paragraph{Detection} Detector treats each frame as an independent one and scans a full image. A scanning window is used which is gradually scaled (in order for the detector to achieve scale invariance) and iteratively shifted along a regular grid of candidate positions. Since this task is computationally intensive a cascade classifier is used so that the detector could quickly decide whether a given subregion contains the object. Therefore a cascade classifier is used. In case of TLD it consists of three sequential stages specifically ordered so that earlier the stage is the more subregions it should reduce while being computationally less expensive. Should the sub-region be rejected by any stage, later stages ignore it completely.

\paragraph{Tracking} The tracking sub-system is based on thew algorithm called the Median-Flow tracker. A $10 \times 10$ grid is used to select the positions within the bounding box representing the object. For each position a given point is tracked between the consecutive frames using pyramidal Lucas-Kanade tracker and eventually the tracker only accepts a 50\% of the most reliable displacements to estimate a new position of the target.

\paragraph{Learning} Since the classifier used in the detection phase is initially trained using only one positive patch (the initial bounding box selected by a user) it tends to make errors as a video stream progress since the moving object of interest change its appearance due to the transformation caused by its motion. Therefore an online so called \texttt{P-N learning} component is incorporated in the system which gradually extends the training sets. Two experts are used, a \texttt{P-expert} identifies only false negatives while \texttt{N-expert} identifies only false positives. Once a wrongly classified patch is found, the experts extend the training set and the classifier is retrained (see Figure \ref{fig:tld_pn_learning_block_diagram}).

%=========================================================================
%=========================================================================
\chapter{Cooperation among camera units}
% 1 - 4 ns

- vyber druhe jednotky pro trackovani cile
- najezd po opticke ose
- predani cile


%=========================================================================
%=========================================================================
\chapter{Target localization}
% 2,5 - 6 ns

- triangulace
- === LOKALIZACE OBJEKTU VE VICEKAMEROVYCH SYSTEMECH ===
- === ALGORITMY VHODNE PRO RESENI ULOHY ===

- Cameras are modeled as pinhole cameras. This is a simple model that describes the mathematical
relationship between the coordinates of a 3D point in the camera coordinate system and its
projection onto the image plane in an ideal camera without lenses through the expressions in 

== (obrazek z rvizu (protnuti primek) ==

 Given  the  extrinsic  and  intrinsic  camera 
 parameters,  each  image  point  defines  a  ray  in  three 
 dimensional spaces, and in the absence of measurement errors, 
 all  rays  from  different  cameras  intersect  in  the  same  object 
 point. But actually the four rays may not intersect in the same 
 point  due  to  the  low-quality  video  cameras  and  other 
 complicated  reasons.
 


%=========================================================================
%=========================================================================
\chapter{Implementation} \label{txt:implementation}
% 2 - 4 ns

The whole system is built on the robotic framework Robot Operating System (for details see Section \ref{txt:robot_operating_system}). Since the ROS defines multiple conventions, restrictions and best practices the whole system design including the selection of a programing language, a programing and a communication paradigm, a target platform and a tool for physical simulations is impacted by the possibilities of this framework.

As of writing this text a current state of the implementation mainly builds on the virtual environment provided by the physical simulator Gazebo (see Section \ref{txt:application_of_gazebo}). When confronted with the overview of all subsystems making up the whole system presented in Section \ref{txt:system_overview} so far the following parts are already designed, implemented and/or integrated:

\begin{itemize}
\item manual control of the manipulator using peripheral devices
\item manual selection of a target and distribution of its appearance to all CUs
\item integration and utilization of a OpenTLD tracker
\item triangulation of a 3D positions of a target within global frame
\item integration of all subsystems
\end{itemize}

Furthermore a few additional tools were utilized and/or implemented as the necessary building blocks allowing for further development and testing:

\begin{itemize}
\item a functional model of a whole system in Gazebo environment
\item a standalone application for rectification
\end{itemize}


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Robot Operating System} \label{txt:robot_operating_system}

Despite its name the ROS\footnote{The official website of ROS: \url{http://www.ros.org}} is not an operating system but rather a collection of open source libraries, tools and conventions which serve the purpose of a middlewear running alongside a real operating system, however it provides the programmer with the hardware abstraction,  low-level device control, implementation of commonly-used functionality,
message-passing between processes, and package management \cite{O'Kane201310}.

Since the original motivation for developing ROS was to support the collaboration among the experts in the field of robotics in the means of a common software platform \cite{ROS-an-open-source-Robot-Operating-System}, a huge developer community has formed around ROS which resulted in wide-scale penetration of this framework as well as the support for a huge range of hardware devices.

%.........................................................................
%.........................................................................
\subsection{Application of ROS}

The OLS is designed to become a relatively complex system, thus it exhibits non-trivial implementation requirements such as a need to distribute the computation among multiple computers, the real time performance, integration with physical simulator, etc. Since ROS is a mature framework satisfying the most of theses requirements (see Table \ref{tab:ols_requirements_ros_features}), it was chosen as a main implementation platform.


{\renewcommand{\arraystretch}{1.5}
\begin{table}[htbp]
	\centering
	\caption{The table lists the most important requirements of the OLS and describes how the ROS framework addresses them.}
	\begin{tabularx}{0.99\textwidth}{XX}
		\toprule
		\textbf{OLS requirements} & \textbf{ROS features} \\
		\midrule
		native support for hardware such as Prosilica camera, manipulator Flir PTU D46-70, joystick, keyboard & nodes implementing image capture from Prosilica cameras, capturing events from keyboards and joysticks \\
		modularity and reusability of source code & each subsystem is represented by a separate process (node), straightforward reusability \\
		distribution of computation among multiple computers & provides abstraction layer for distributing nodes across devices \\
		simple data exchange among subsystems & the publisher subscriber paradigm \cite{O'Kane201310}, support for custom message formats \\
		real time performance & C++ implementation \\
		modelling and simulating the robot & custom language \texttt{URDF}\footnotemark ~for robot modeling, integration with Gazebo \\
		specifying a kinematic chain, heavy 3D transformation computation & native support for computing transformation between frames using package \texttt{tf} \\
		complex visualization, debugging & a visualization tool \texttt{rviz}\footnotemark ~for visualizing frames, transformations, robot models, image streams etc. \\
		physical simulation & integration with Gazebo \\
		\bottomrule
	\end{tabularx}
	\label{tab:ols_requirements_ros_features}
\end{table}}

\footnotetext{Modeling language \texttt{URDF}: \url{http://wiki.ros.org/urdf}}
\footnotetext{Vsisualization tool \texttt{rviz}: \url{http://wiki.ros.org/rviz}}

%.........................................................................
%.........................................................................
\subsection{Standard ROS packages}

ROS provides a wide range of standard packages for interaction with various hardware devices and performing various computations. The implementation of OLS utilizes following packages:

\begin{description}
	\item[avt\_vimba\_camera]\footnote{Package avt\_vimba\_camera: \url{http://wiki.ros.org/avt_vimba_camera}} \hfill \\
	This package wraps the Vimba GigE SDK provided by Allied Vision Technologies, the manufacturer of the Prosilica series cameras, and allows the programmer to subscribe to the topic \texttt{camera\textbackslash image\_raw} and easily access the image stream.
	
	\item[keyboard]\footnote{Package keyboard: \url{http://wiki.ros.org/keyboard}} \hfill \\
	The package process the keyboard events and expose them via \texttt{keydown} and \texttt{keyup} topics.
	
	\item[joy]\footnote{Package joy: \url{http://wiki.ros.org/joy}} \hfill \\
	This package process the events from a joystick and/or gamepad and expose them via \texttt{joy} topic.
	
	\item[tf]\footnote{Package tf: \url{http://wiki.ros.org/tf}} \hfill \\
	The package manages the distribution of the states of all joints within all robot models (in case of OLS the kinematic chains representing the camera units) among all nodes as well as it performs the transformation between given frames on demand \cite{tf}.
	
\end{description}
	

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{System architecture}

Given the modular concept of the ROS it is most convenient to divide the whole system into autonomous subsystems represented by separate ROS packages. From the perspective of OLS this division of computation work among packages should loosely correspond to the hardware units used.

%.........................................................................
%.........................................................................
\subsection{Hardware} \label{txt:hardware}

Considering the big picture of the system, OLS consists of four camera stations, a hardware devices necessary for network communication and the peripherals for manual control. There are two types of camera stations, a tracking station and an overview station (see Section \ref{txt:system_overview}). All stations communicate with each other vie Ethernet switch. The peripheral devices (keyboard, joystick) allowing the operator to manually control the system are connected to the overview station (see Figure \ref{fig:hw_ols}).

%% The big picture diagram of the hardware components of the OLS
\begin{figure}[htb]
	\centering
	\includegraphics[width=10cm]{fig/hw_ols.pdf}
	\caption{The big picture diagram of the main components of the OLS.}
	\label{fig:hw_ols}
\end{figure}

A camera station itself consists of a camera unit (see Section \ref{txt:camera_unit}) and a computer running the OLS software. The camera unit is controlled by the controller \texttt{STM32F4007} which communicates with the manipulator \texttt{Flir PTU D46-70}, a GPS module and a zoom lense. The camera \texttt{Prosilica GT 1290C} is connected directly to a computer running OLS system (for more details see Figure \ref{fig:hw_camera_unit}).

%% The diagram of the hardware components of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=10cm]{fig/hw_camera_unit.pdf}
	\caption{The diagram of the hardware components of the camera unit depicting both the hardware devices and the communication standards.}
	\label{fig:hw_camera_unit}
\end{figure}



%.........................................................................
%.........................................................................
\subsection{Software} \label{txt:software}

The system is divided into multiple ROS nodes (processes running in the operating system) with the aim to loosely resemble the hardware components. Five namespaces are used, one \texttt{master} namespace and four \texttt{camera\_unit\_N} namespaces, where $N \in \{0, 1, 2, 3\}$ identifies a unique camera unit. The overview of the system architecture from the perspective of the ROS namespaces, nodes, messages and services is depicted on Figure \ref{fig:sw_ols}.

%% The diagram of the software components - ROS nodes - and ROS topics.
\begin{figure}[htb]
	\centering
	\includegraphics[width=15.5cm]{fig/sw_ols.pdf}
	\caption{The diagram of the software components represented by the ROS nodes. The communication among topics is implemented using ROS topics (normal arrows) and ROS services (dashed arrows).}
	\label{fig:sw_ols}
\end{figure}

The nodes running within the \texttt{master} namespace serve the purpose of the access point for an user (an operator) as well as the main controller of the whole system. The node \texttt{controls} keeps the information about the tracked targets and distributes the workload to the separate camera units based on their position and orientation with regards to the newly discovered target. The node \texttt{position\_estimation} calculates the triangulation and estimates the 3D position of the target. The node \texttt{GUI} implements the graphical user interface allowing the operator to control the system and the nodes \texttt{joy} and \texttt{keyboard} process the events from peripherals.

Each of the namespaces \texttt{camera\_unit\_N} control a separate camera unit. The namespace contains the standard node \texttt{avt\_vimba\_camera} processing the input image stream from Prosilica camera, and implements the node \texttt{manipulator} which controls the manipulator and publishes its state and the node \texttt{detection\_and\_tracking} which performs the computationally most expensive tasks of the object detection, learning its appearance and tracking.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Application of Gazebo} \label{txt:application_of_gazebo}

Gazebo\footnote{The official website of Gazebo: \url{http://gazebosim.org}} is a physical simulator developed by the OSRF\footnote{Open Source Robotic Foundation: \url{http://www.osrfoundation.org/}} providing the tools to model and simulate robots in both indoor and outdoor environment. The simulator has been developed since 2002 and today represents a mature system with wide penetration and support, while being distributed as open source and freeware. 

Since the Gazebo is distributed also as one of the standard packages of the ROS framework it is straightforward to integrate the simulation environment with the already implemented ROS nodes. There are multiple advantages of using the simulator over a development using a real hardware, the main motivations were as follows:

\paragraph{testing a tracker} The Gazebo provides the ROS plugin simulating an RGB camera, which captures the virtual scene and publishes a stream of rendered images. Thus it can be used to test an object tracking algorithm using arbitrarily complex environment and moving objects (see Figure \ref{fig:gazebo_camera_stream}).

%% The screenshot of Gazebo scene and image streams from cameras.
\begin{figure}[htb]
	\centering
	\includegraphics[width=15.5cm]{fig/gazebo_camera_stream.png}
	\caption{The screenshot of a Gazebo simulation (left) consisting of a virtual environment (the gas station), a flying object (the red sphere) and four manipulators. All four virtual camera streams are displayed real-time using \texttt{rviz} tool (right).}
	\label{fig:gazebo_camera_stream}
\end{figure}

\paragraph{testing a manipulator} It is a good practice to include a real hardware in the simulation during the development \cite{on_hw_in_the_loop}. Both actuators and sensors would be difficult to simulate properly, moreover a real manipulator is constrained in terms of the operational range (see Section \ref{txt:camera_unit}), maximum acceleration and speed and communication throughput so it is necessary to thoroughly test its performance. This so called hardware-in-the-loop simulation reveals whether the implementation of motion control is correct and whether the possibilities of the manipulator suffice to track arbitrarily fast (simulated) objects.

\paragraph{testing a triangulation} Thanks to the Gazebo it is possible to simulate a flying object with a-priory set trajectory and evaluate the precision of a position estimation algorithm using comparison between the estimated target position and a ground-truth.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{External libraries} \label{txt:external_libraries}

Besides the framework ROS a few other publicly available libraries are used within the implementation.

\paragraph{OpenCV} Open Source Computer Vision Library\footnote{The official website of OpenCV: \url{http://opencv.org}} is a free open source library providing algorithms for image processing, computer vision and machine learning. Version 2.4.11 is used as it is a component of ROS Indigo.

\paragraph{Eigen} This open source C++ template library\footnote{The official website of Eigen: \url{http://eigen.tuxfamily.org}} implements the data structures and methods for fast and convenient solving of linear algebra problems.

\paragraph{OpenTLD} The OpenTLD library\footnote{The official website of OpenTLD: \url{http://www.gnebehay.com/tld}} represents an open source C++ implementation the TLD tracking algorithm (see Section \ref{txt:detection_and_tracking}).	

\paragraph{Serial} A cross-platform library Serial\footnote{The official website of Serial: \url{https://github.com/wjwwood/serial}} implemented in C++ provides the API for interfacing with RS-232 serial like ports. It is used to control the manipulator.



%=========================================================================
%=========================================================================
\chapter{Experiments and results}
% 1 - 3 ns

- vlastni datova sada
- experimenty na ziskena a porizene datove sade
- === EXPERIMENTY NA REALNYCH DATECH ===

%=========================================================================
%=========================================================================
\chapter{Conclusion}
% 1 - 2 ns

- zhodnocení dosažených výsledků se zvlášť vyznačeným vlastním přínosem studenta
- zhodnocení z pohledu dalšího vývoje projektu vzhledem k diplomové práci
- moznosti dalsiho vyvoje: modelovani okoli ve 3D nebo vyuziti teto info z mapy, paralelizace narocnych casti na GPU
- veci, co se zatim nebraly v uvahu
	- system je umisten na rozhrani dvou a vice UTM zon

- moznost sdileni info o appearance modelu mezi CUs
- vylepseni TLD, protoze prepokladame fixni scenu (meni se pouze natoceni kamery)


